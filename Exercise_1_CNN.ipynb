{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_1_CNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MianUsmanAhmed12/Lab7/blob/main/Exercise_1_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwRGEQbzGpYQ"
      },
      "source": [
        "# First CNN model for MNIST Dataset\n",
        "\n",
        "* MNIST Dataset is ''Hello World'' of Image Recognition\n",
        "\n",
        "* [Dataset HomePage](http://yann.lecun.com/exdb/mnist/)\n",
        "\n",
        "* History of MNIST Dataset [Watch here](https://www.youtube.com/watch?v=oKzNUGz21JM)\n",
        "\n",
        "\n",
        "---\n",
        "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a \n",
        "test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
        "\n",
        "![Kitten](https://camo.githubusercontent.com/01c057a753e92a9bc70b8c45d62b295431851c09cffadf53106fc0aea7e2843f/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhmJOHCpJD_w"
      },
      "source": [
        "# Task 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSyHCSV7jymI"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWNzCYUUjymN"
      },
      "source": [
        "Importantly, a convnet takes as input tensors of shape (image_height, image_width,\n",
        "image_channels) (not including the batch dimension). In this case, we’ll configure\n",
        "the convnet to process inputs of size (28, 28, 1), which is the format of MNIST\n",
        "images. We’ll do this by passing the argument input_shape=(28, 28, 1) to the first\n",
        "layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM4JLEpwjymN"
      },
      "source": [
        "#### Instantiating a small convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-OnpExGjymO",
        "outputId": "8392b0db-f0be-4f83-e019-0dffd4a5648c"
      },
      "source": [
        "from keras.layers import LeakyReLU\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3),activation='relu', input_shape=(28, 28, 1)))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3),activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3),activation='relu' ) )\n",
        "         \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gcVG3xkjymR"
      },
      "source": [
        "#### Adding a classifier on top of the convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2DfhDJYjymR",
        "outputId": "1b8f9f3b-a6f9-4c22-ed9b-71f9d50a39ff"
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOKVF4nKjymU"
      },
      "source": [
        "### Training the convnet on MNIST images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIcgUbbUjymV"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnJ2Pfs_jymX"
      },
      "source": [
        "#### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpHGHE9MjymY"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HoTLrfSjymd"
      },
      "source": [
        "#### compile and fit model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i23FDtC9jyme",
        "outputId": "47c9f2ec-c8e6-462d-f8e5-ad1a339e09e3"
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "750/750 [==============================] - 112s 149ms/step - loss: 0.2302 - accuracy: 0.9302 - val_loss: 0.1006 - val_accuracy: 0.9679\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 105s 140ms/step - loss: 0.0721 - accuracy: 0.9768 - val_loss: 0.0706 - val_accuracy: 0.9793\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 91s 121ms/step - loss: 0.0534 - accuracy: 0.9832 - val_loss: 0.0509 - val_accuracy: 0.9852\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 113s 151ms/step - loss: 0.0414 - accuracy: 0.9868 - val_loss: 0.0571 - val_accuracy: 0.9822\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 102s 136ms/step - loss: 0.0362 - accuracy: 0.9885 - val_loss: 0.0468 - val_accuracy: 0.9861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zU8iI5ojymg"
      },
      "source": [
        "#### evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3VeaL1Njymh",
        "outputId": "63082eae-02a9-40eb-e233-6c27ee8a8b27"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "test_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 6s 20ms/step - loss: 0.0337 - accuracy: 0.9886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9886000156402588"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3UWMwiQF8wD"
      },
      "source": [
        "#after changing optimizer to adam our accuracy was increased to 0.9896000027656555\n",
        "#after changing activation to tanh accuracy changed to 0.9883000254631042\n",
        "#i tried leakyrelu and linear but their accuracies were very low\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P7VxZB_F8wD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}